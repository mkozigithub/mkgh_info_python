{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing from SAS using SAS7BDAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sas7bdat import SAS7BDAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[posts-100.sas7bdat] header length 65536 != 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAS7BDAT file: posts-100.sas7bdat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with SAS7BDAT('./files/posts-100.sas7bdat') as sas_file:\n",
    "    print(sas_file)\n",
    "    sas_df = sas_file.to_data_frame()\n",
    "    \n",
    "type(sas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>Id</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1243.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AcceptedAnswerId  AnswerCount  CommentCount  FavoriteCount    Id  \\\n",
       "0               NaN          1.0           1.0            1.0   5.0   \n",
       "1              10.0          3.0           4.0            1.0   7.0   \n",
       "2               NaN          NaN           0.0            NaN   9.0   \n",
       "3               NaN          NaN           1.0            NaN  10.0   \n",
       "4              29.0          4.0           1.0            4.0  14.0   \n",
       "\n",
       "   LastEditorUserId  OwnerUserId  ParentId  PostTypeId  Score  ViewCount  \n",
       "0               NaN          5.0       NaN         1.0    9.0      448.0  \n",
       "1              97.0         36.0       NaN         1.0    4.0      388.0  \n",
       "2               NaN         51.0       5.0         2.0    5.0        NaN  \n",
       "3               NaN         22.0       7.0         2.0   12.0        NaN  \n",
       "4             322.0         66.0       NaN         1.0   21.0     1243.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AcceptedAnswerId', 'AnswerCount', 'CommentCount', 'FavoriteCount',\n",
       "       'Id', 'LastEditorUserId', 'OwnerUserId', 'ParentId', 'PostTypeId',\n",
       "       'Score', 'ViewCount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Header:\n",
       "\tcol_count_p1: 11\n",
       "\tcol_count_p2: 0\n",
       "\tcolumn_count: 11\n",
       "\tcompression: None\n",
       "\tcreator: None\n",
       "\tcreator_proc: None\n",
       "\tdate_created: 2018-10-12 21:26:30.506552\n",
       "\tdate_modified: 2018-10-12 21:26:30.506552\n",
       "\tendianess: little\n",
       "\tfile_type: DATA\n",
       "\tfilename: posts-100.sas7bdat\n",
       "\theader_length: 65536\n",
       "\tlcp: 8\n",
       "\tlcs: 0\n",
       "\tmix_page_row_count: 706\n",
       "\tname: IMPORT2\n",
       "\tos_name: x86_64\n",
       "\tos_type: 2.6.32-696.20.1.\n",
       "\tpage_count: 1\n",
       "\tpage_length: 65536\n",
       "\tplatform: unix\n",
       "\trow_count: 100\n",
       "\trow_length: 88\n",
       "\tsas_release: 9.0401M5\n",
       "\tserver_type: Linux\n",
       "\tu64: True\n",
       "\n",
       "Contents of dataset \"IMPORT2\":\n",
       "Num Name                Type   Length Format Label\n",
       "--- ------------------- ------ ------ ------ -----\n",
       "  1 AcceptedAnswerId    number      8 BEST        \n",
       "  2 AnswerCount         number      8 BEST        \n",
       "  3 CommentCount        number      8 BEST        \n",
       "  4 FavoriteCount       number      8 BEST        \n",
       "  5 Id                  number      8 BEST        \n",
       "  6 LastEditorUserId    number      8 BEST        \n",
       "  7 OwnerUserId         number      8 BEST        \n",
       "  8 ParentId            number      8 BEST        \n",
       "  9 PostTypeId          number      8 BEST        \n",
       " 10 Score               number      8 BEST        \n",
       " 11 ViewCount           number      8 BEST        "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas_file.header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COMPRESSION_LITERALS',\n",
       " 'DATE_FORMAT_STRINGS',\n",
       " 'DATE_TIME_FORMAT_STRINGS',\n",
       " 'DECOMPRESSORS',\n",
       " 'RDC_COMPRESSION',\n",
       " 'RLE_COMPRESSION',\n",
       " 'TIME_FORMAT_STRINGS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_file',\n",
       " '_iter',\n",
       " '_make_logger',\n",
       " '_open_files',\n",
       " '_process_byte_array_with_data',\n",
       " '_read_bytes',\n",
       " '_read_next_page',\n",
       " '_read_val',\n",
       " '_update_format_strings',\n",
       " 'align_correction',\n",
       " 'cached_page',\n",
       " 'close',\n",
       " 'column_data_lengths',\n",
       " 'column_data_offsets',\n",
       " 'column_names',\n",
       " 'column_names_strings',\n",
       " 'column_types',\n",
       " 'columns',\n",
       " 'convert_file',\n",
       " 'current_file_position',\n",
       " 'current_page_block_count',\n",
       " 'current_page_data_subheader_pointers',\n",
       " 'current_page_subheaders_count',\n",
       " 'current_page_type',\n",
       " 'current_row',\n",
       " 'encoding',\n",
       " 'encoding_errors',\n",
       " 'endianess',\n",
       " 'header',\n",
       " 'logger',\n",
       " 'path',\n",
       " 'properties',\n",
       " 'readlines',\n",
       " 'skip_header',\n",
       " 'to_data_frame',\n",
       " 'u64']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sas_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing SAS using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>Id</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1243.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AcceptedAnswerId  AnswerCount  CommentCount  FavoriteCount    Id  \\\n",
       "0               NaN          1.0           1.0            1.0   5.0   \n",
       "1              10.0          3.0           4.0            1.0   7.0   \n",
       "2               NaN          NaN           0.0            NaN   9.0   \n",
       "3               NaN          NaN           1.0            NaN  10.0   \n",
       "4              29.0          4.0           1.0            4.0  14.0   \n",
       "\n",
       "   LastEditorUserId  OwnerUserId  ParentId  PostTypeId  Score  ViewCount  \n",
       "0               NaN          5.0       NaN         1.0    9.0      448.0  \n",
       "1              97.0         36.0       NaN         1.0    4.0      388.0  \n",
       "2               NaN         51.0       5.0         2.0    5.0        NaN  \n",
       "3               NaN         22.0       7.0         2.0   12.0        NaN  \n",
       "4             322.0         66.0       NaN         1.0   21.0     1243.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_sas = pd.read_sas('./files/posts-100.sas7bdat')\n",
    "posts_sas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AcceptedAnswerId', 'AnswerCount', 'CommentCount', 'FavoriteCount',\n",
       "       'Id', 'LastEditorUserId', 'OwnerUserId', 'ParentId', 'PostTypeId',\n",
       "       'Score', 'ViewCount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_sas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.sas.sas7bdat.SAS7BDATReader'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>Id</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AcceptedAnswerId  AnswerCount  CommentCount  FavoriteCount   Id  \\\n",
       "0               NaN          1.0           1.0            1.0  5.0   \n",
       "1              10.0          3.0           4.0            1.0  7.0   \n",
       "2               NaN          NaN           0.0            NaN  9.0   \n",
       "\n",
       "   LastEditorUserId  OwnerUserId  ParentId  PostTypeId  Score  ViewCount  \n",
       "0               NaN          5.0       NaN         1.0    9.0      448.0  \n",
       "1              97.0         36.0       NaN         1.0    4.0      388.0  \n",
       "2               NaN         51.0       5.0         2.0    5.0        NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in chunks - useful for very large files\n",
    "posts_sas_reader = pd.read_sas('./files/posts-100.sas7bdat', chunksize=3)\n",
    "print(type(posts_sas_reader))\n",
    "chunk = posts_sas_reader.read()\n",
    "print(type(chunk))\n",
    "chunk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Stata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount',\n",
      "       'Body', 'OwnerUserId', 'LastActivityDate', 'Title', 'Tags',\n",
      "       'AnswerCount', 'CommentCount', 'FavoriteCount', 'ClosedDate',\n",
      "       'AcceptedAnswerId', 'LastEditorUserId', 'LastEditDate', 'ParentId',\n",
      "       'CommunityOwnedDate', 'LastEditorDisplayName', 'OwnerDisplayName'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>...</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "      <th>OwnerDisplayName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-05-13T23:58:30.457</td>\n",
       "      <td>9</td>\n",
       "      <td>448</td>\n",
       "      <td>&lt;p&gt;I've always been interested in machine lear...</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-05-14T00:36:31.077</td>\n",
       "      <td>How can I do simple machine learning without h...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-05-14T14:40:25.950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-05-14T00:11:06.457</td>\n",
       "      <td>4</td>\n",
       "      <td>388</td>\n",
       "      <td>&lt;p&gt;As a researcher and instructor, I'm looking...</td>\n",
       "      <td>36</td>\n",
       "      <td>2014-05-16T13:45:00.237</td>\n",
       "      <td>What open-source books (or other materials) pr...</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-05-14T08:40:54.950</td>\n",
       "      <td>10</td>\n",
       "      <td>97</td>\n",
       "      <td>2014-05-16T13:45:00.237</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-05-14T00:36:31.077</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;Not sure if this fits the scope of this SE,...</td>\n",
       "      <td>51</td>\n",
       "      <td>2014-05-14T00:36:31.077</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-05-14T00:53:43.273</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;One book that's freely available is \"The El...</td>\n",
       "      <td>22</td>\n",
       "      <td>2014-05-14T00:53:43.273</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-05-14T01:25:59.677</td>\n",
       "      <td>21</td>\n",
       "      <td>1243</td>\n",
       "      <td>&lt;p&gt;I am sure data science as will be discussed...</td>\n",
       "      <td>66</td>\n",
       "      <td>2014-06-20T17:36:05.023</td>\n",
       "      <td>Is Data Science the Same as Data Mining?</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>29</td>\n",
       "      <td>322</td>\n",
       "      <td>2014-06-17T16:17:20.473</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Id  PostTypeId             CreationDate  Score  ViewCount  \\\n",
       "0      0   5           1  2014-05-13T23:58:30.457      9        448   \n",
       "1      1   7           1  2014-05-14T00:11:06.457      4        388   \n",
       "2      2   9           2  2014-05-14T00:36:31.077      5          0   \n",
       "3      3  10           2  2014-05-14T00:53:43.273     12          0   \n",
       "4      4  14           1  2014-05-14T01:25:59.677     21       1243   \n",
       "\n",
       "                                                Body  OwnerUserId  \\\n",
       "0  <p>I've always been interested in machine lear...            5   \n",
       "1  <p>As a researcher and instructor, I'm looking...           36   \n",
       "2  <p>Not sure if this fits the scope of this SE,...           51   \n",
       "3  <p>One book that's freely available is \"The El...           22   \n",
       "4  <p>I am sure data science as will be discussed...           66   \n",
       "\n",
       "          LastActivityDate                                              Title  \\\n",
       "0  2014-05-14T00:36:31.077  How can I do simple machine learning without h...   \n",
       "1  2014-05-16T13:45:00.237  What open-source books (or other materials) pr...   \n",
       "2  2014-05-14T00:36:31.077                                                      \n",
       "3  2014-05-14T00:53:43.273                                                      \n",
       "4  2014-06-20T17:36:05.023           Is Data Science the Same as Data Mining?   \n",
       "\n",
       "        ...        CommentCount  FavoriteCount               ClosedDate  \\\n",
       "0       ...                   1              1  2014-05-14T14:40:25.950   \n",
       "1       ...                   4              1  2014-05-14T08:40:54.950   \n",
       "2       ...                   0              0                            \n",
       "3       ...                   1              0                            \n",
       "4       ...                   1              4                            \n",
       "\n",
       "   AcceptedAnswerId LastEditorUserId             LastEditDate  ParentId  \\\n",
       "0                 0                0                                  0   \n",
       "1                10               97  2014-05-16T13:45:00.237         0   \n",
       "2                 0                0                                  5   \n",
       "3                 0                0                                  7   \n",
       "4                29              322  2014-06-17T16:17:20.473         0   \n",
       "\n",
       "  CommunityOwnedDate  LastEditorDisplayName OwnerDisplayName  \n",
       "0                                                             \n",
       "1                                                             \n",
       "2                                                             \n",
       "3                                                             \n",
       "4                                                             \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_stata('./files/posts-100.dta')\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing HDF5 data with H5PY and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using h5py to import HDF5 data\n",
    "import h5py\n",
    "\n",
    "h5_file = h5py.File('./files/posts-100.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'h5py._hl.group.Group'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/posts\" (2 members)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_h5 = h5_file['posts']\n",
    "print(type(dataset_h5))\n",
    "dataset_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 5, 1, b'2014-05-13T23:58:30.457', 9, 448, b'<p>I\\'ve always been interested in machine learning, but I can\\'t figure out one thing about starting out with a simple \"Hello World\" example - how can I avoid hard-coding behavior?</p><p>For example, if I wanted to \"teach\" a bot how to avoid ran', 5, b'2014-05-14T00:36:31.077', b'How can I do simple machine learning without hard-coding behavior?', b'<machine-learning>', 1, 1, 1, b'2014-05-14T14:40:25.950', 0, 0, b'', 0, b'', b'', b'')\n",
      "(1, 7, 1, b'2014-05-14T00:11:06.457', 4, 388, b\"<p>As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I'm especially interested in a thorough overview t\", 36, b'2014-05-16T13:45:00.237', b'What open-source books (or other materials) provide a relatively thorough overview of data science?', b'<education><open-source>', 3, 4, 1, b'2014-05-14T08:40:54.950', 10, 97, b'2014-05-16T13:45:00.237', 0, b'', b'', b'')\n",
      "(2, 9, 2, b'2014-05-14T00:36:31.077', 5, 0, b\"<p>Not sure if this fits the scope of this SE, but here's a stab at an answer anyway.</p><p>With all AI approaches you have to decide what it is you're modelling and what kind of uncertainty there is. Once you pick a framework that allows model\", 51, b'2014-05-14T00:36:31.077', b'', b'', 0, 0, 0, b'', 0, 0, b'', 5, b'', b'', b'')\n",
      "(3, 10, 2, b'2014-05-14T00:53:43.273', 12, 0, b'<p>One book that\\'s freely available is \"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman (published by Springer): <a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/\">see Tibshirani\\'s website</a>.</p><p>Another fa', 22, b'2014-05-14T00:53:43.273', b'', b'', 0, 1, 0, b'', 0, 0, b'', 7, b'', b'', b'')\n",
      "(4, 14, 1, b'2014-05-14T01:25:59.677', 21, 1243, b'<p>I am sure data science as will be discussed in this forum has several synonyms or at least related fields where large data is analyzed.</p><p>My particular question is in regards to Data Mining.  I took a graduate class in Data Mining a few ', 66, b'2014-06-20T17:36:05.023', b'Is Data Science the Same as Data Mining?', b'<data-mining><definitions>', 4, 1, 4, b'', 29, 322, b'2014-06-17T16:17:20.473', 0, b'', b'', b'')\n",
      "(5, 15, 1, b'2014-05-14T01:41:23.110', 2, 543, b'<p>In which situations would one system be preferred over the other? What are the relative advantages and disadvantages of relational databases versus non-relational databases?</p>', 64, b'2014-05-14T01:41:23.110', b'What are the advantages and disadvantages of SQL versus NoSQL in data science?', b'<databases>', 0, 1, 0, b'2014-05-14T07:41:49.437', 0, 0, b'', 0, b'', b'', b'')\n",
      "(6, 16, 1, b'2014-05-14T01:57:56.880', 18, 322, b'<p>I use <a href=\"http://www.csie.ntu.edu.tw/~cjlin/libsvm/\">Libsvm</a> to train data and predict classification on <strong>semantic analysis</strong> problem. But it has a <strong>performance</strong> issue on large-scale data, because semanti', 63, b'2014-05-17T16:24:14.523', b'Use liblinear on big data for semantic analysis', b'<machine-learning><bigdata><libsvm>', 2, 0, 0, b'', 46, 84, b'2014-05-17T16:24:14.523', 0, b'', b'', b'')\n",
      "(7, 17, 5, b'2014-05-14T02:49:14.580', 0, 0, b'<p><a href=\"http://www.csie.ntu.edu.tw/~cjlin/libsvm/\" rel=\"nofollow\">LIBSVM</a> is a library for support vector classification (SVM) and regression.It was created by Chih-Chung Chang and Chih-Jen Lin in 2001.</p>', 63, b'2014-05-16T13:44:53.470', b'', b'', 0, 0, 0, b'', 0, 63, b'2014-05-16T13:44:53.470', 0, b'', b'', b'')\n",
      "(8, 18, 4, b'2014-05-14T02:49:14.580', 0, 0, b'', -1, b'2014-05-14T02:49:14.580', b'', b'', 0, 0, 0, b'', 0, -1, b'2014-05-14T02:49:14.580', 0, b'', b'', b'')\n",
      "(9, 19, 1, b'2014-05-14T03:56:20.963', 73, 7993, b'<p>Lots of people use the term <em>big data</em> in a rather <em>commercial</em> way, as a means of indicating that large datasets are involved in the computation, and therefore potential solutions must have good performance. Of course, <em>big', 84, b'2018-05-01T13:04:43.563', b'How big is big data?', b'<bigdata><scalability><efficiency><performance>', 12, 5, 20, b'', 37, 10119, b'2015-06-11T20:15:28.720', 0, b'', b'', b'')\n",
      "(10, 20, 1, b'2014-05-14T05:37:46.780', 17, 311, b\"<p>we created this social network application for eLearning purposes, it's an experimental thing we are researching on in our lab. it has been used by some case studies for a while and the data on our relational DBMS (SQL Server 2008) is gettin\", 96, b'2017-08-29T11:26:37.137', b'the data on our relational DBMS is getting big, is it the time to move to NoSQL?', b'<nosql><relational-dbms>', 5, 1, 1, b'', 26, 0, b'', 0, b'', b'', b'')\n",
      "(11, 21, 2, b'2014-05-14T05:44:29.340', 29, 0, b'<p>As you rightly note, these days \"big data\" is something everyone wants to say they\\'ve got, which entails a certain looseness in how people define the term.  Generally, though, I\\'d say you\\'re certainly dealing with big data if the scale is su', 14, b'2014-05-14T05:44:29.340', b'', b'', 0, 0, 0, b'', 0, 0, b'', 19, b'', b'', b'')\n",
      "(12, 22, 1, b'2014-05-14T05:58:21.927', 99, 105748, b'<p>My data set contains a number of numeric attributes and one categorical.</p><p>Say, <code>NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr</code>, </p><p>where <code>CategoricalAttr</code> takes one of three possible values: <c', 97, b'2018-07-11T11:22:26.170', b'K-Means clustering for mixed numeric and categorical data', b'<data-mining><clustering><octave><k-means><categorical-data>', 9, 3, 80, b'', 24, 97, b'2014-06-10T07:53:48.253', 0, b'', b'', b'')\n",
      "(13, 23, 2, b'2014-05-14T06:06:13.603', 8, 0, b'<p>Data Science specialization from Johns Hopkins University at Coursera would be a great start.<a href=\"https://www.coursera.org/specialization/jhudatascience/1\">https://www.coursera.org/specialization/jhudatascience/1</a></p>', 97, b'2014-05-14T06:06:13.603', b'', b'', 0, 0, 0, b'', 0, 0, b'', 7, b'', b'', b'')\n",
      "(14, 24, 2, b'2014-05-14T06:26:27.163', 89, 0, b'', 14, b'2016-11-29T20:06:51.543', b'', b'', 0, 9, 0, b'', 0, 14, b'2016-11-29T20:06:51.543', 22, b'', b'', b'')\n",
      "(15, 25, 2, b'2014-05-14T07:26:04.390', 7, 0, b\"<p>Big Data is defined by the volume of data, that's right, but not only. The particularity of big data is that you need to store a <strong>lots</strong> of <strong>various</strong> and sometimes <strong>unstructured</strong> stuffs <strong>all\", 104, b'2014-05-14T07:26:04.390', b'', b'', 0, 0, 0, b'', 0, 0, b'', 19, b'', b'', b'')\n",
      "(16, 26, 2, b'2014-05-14T07:38:31.103', 14, 0, b'<p>A few gigabytes is not very \"<strong>big</strong>\". It\\'s more like the normal size of an enterprise DB. As long as you go over PK when joining tables it should work out really well, even in the future (as long as you don\\'t get TB\\'s of data a', 115, b'2014-05-14T11:03:51.577', b'', b'', 0, 0, 0, b'', 0, 115, b'2014-05-14T11:03:51.577', 20, b'', b'', b'')\n",
      "(17, 27, 2, b'2014-05-14T07:53:02.560', 10, 0, b'<p>To answer this question you have to answer which kind of compromise you can afford. RDBMs implements <a href=\"http://en.wikipedia.org/wiki/ACID\">ACID</a>. This is expensive in terms of resources. There are no NoSQL solutions which are ACID. ', 108, b'2014-05-14T08:03:37.890', b'', b'', 0, 0, 0, b'', 0, 14, b'2014-05-14T08:03:37.890', 20, b'', b'', b'')\n",
      "(18, 28, 2, b'2014-05-14T07:55:40.133', 6, 0, b'<p>There is free ebook \"<a href=\"http://jsresearch.net/\" rel=\"noreferrer\">Introduction to Data Science</a>\" based on <a href=\"/questions/tagged/r\" class=\"post-tag\" title=\"show questions tagged &#39;r&#39;\" rel=\"tag\">r</a> language</p>', 118, b'2014-05-14T07:55:40.133', b'', b'', 0, 0, 0, b'', 0, 0, b'', 7, b'', b'', b'')\n",
      "(19, 29, 2, b'2014-05-14T07:56:34.437', 24, 0, b'<p><a href=\"https://datascience.stackexchange.com/users/36/statsrus\">@statsRus</a> starts to lay the groundwork for your answer in another question <a href=\"https://datascience.stackexchange.com/questions/1/what-characterises-the-difference-bet', 53, b'2014-05-14T07:56:34.437', b'', b'', 0, 0, 0, b'', 0, -1, b'2017-04-13T12:50:41.230', 14, b'', b'', b'')\n",
      "(20, 30, 2, b'2014-05-14T08:03:28.117', 20, 0, b'', 26, b'2014-05-14T18:30:59.180', b'', b'', 0, 3, 0, b'', 0, 26, b'2014-05-14T18:30:59.180', 19, b'', b'', b'')\n",
      "(21, 31, 1, b'2014-05-14T08:38:07.007', 9, 1190, b'<p>I have a bunch of customer profiles stored in a <a href=\"/questions/tagged/elasticsearch\" class=\"post-tag\" title=\"show questions tagged &#39;elasticsearch&#39;\" rel=\"tag\">elasticsearch</a> cluster. These profiles are now used for creation of', 118, b'2014-05-15T05:49:39.140', b'Clustering customer data stored in ElasticSearch', b'<data-mining><clustering>', 1, 4, 1, b'', 72, 24, b'2014-05-15T05:49:39.140', 0, b'', b'', b'')\n",
      "(22, 33, 2, b'2014-05-14T09:34:15.477', 6, 0, b'<p>Is it the time to move to NoSQL will depends on 2 things: </p><ol><li>The nature/structure of your data</li><li>Your current performance</li></ol><p>SQL databases excel when the data is well structured (e.g. when it can be modeled as a table', 132, b'2017-08-29T11:26:37.137', b'', b'', 0, 0, 0, b'', 0, 132, b'2017-08-29T11:26:37.137', 20, b'', b'', b'')\n",
      "(23, 35, 1, b'2014-05-14T09:51:54.753', 18, 346, b'<p>In working on exploratory data analysis, and developing algorithms, I find that most of my time is spent in a cycle of visualize, write some code, run on small dataset, repeat.   The data I have tends to be computer vision/sensor fusion type', 26, b'2014-05-20T03:56:43.147', b'How to scale up algorithm development?', b'<algorithms>', 3, 0, 3, b'', 0, 0, b'', 0, b'', b'', b'')\n",
      "(24, 37, 2, b'2014-05-14T10:41:23.823', 77, 0, b'<p>To me (coming from a relational database background), \"Big Data\" is not primarily about the data size (which is the bulk of what the other answers are so far).</p><p>\"Big Data\" and \"Bad Data\" are closely related. Relational Databases require', 9, b'2018-05-01T13:04:43.563', b'', b'', 0, 4, 0, b'', 0, 51450, b'2018-05-01T13:04:43.563', 19, b'', b'', b'')\n",
      "(25, 38, 1, b'2014-05-14T10:44:58.933', 14, 2985, b'<p>I heard about many tools / frameworks for helping people to process their data (big data environment). </p><p>One is called Hadoop and the other is the noSQL concept. What is the difference in point of processing? </p><p>Are they complementa', 134, b'2015-05-18T12:30:19.497', b'What is the difference between Hadoop and noSQL', b'<nosql><tools><processing><apache-hadoop>', 2, 2, 2, b'', 43, 134, b'2014-05-14T22:26:59.453', 0, b'', b'', b'')\n",
      "(26, 40, 2, b'2014-05-14T11:12:03.880', 8, 0, b'<p>Big Data is actually not so about the \"how big it is\". </p><p>First, few gigabytes is not big at all, it\\'s almost nothing. So don\\'t bother yourself, your system will continu to work efficiently for some time I think.</p><p>Then you have to t', 104, b'2014-05-14T11:12:03.880', b'', b'', 0, 0, 0, b'', 0, 0, b'', 20, b'', b'', b'')\n",
      "(27, 41, 1, b'2014-05-14T11:15:40.907', 43, 4571, b'<p>R has many libraries which are aimed at Data Analysis (e.g. JAGS, BUGS, ARULES etc..), and is mentioned in popular textbooks such as: J.Krusche, Doing Bayesian Data Analysis; B.Lantz, \"Machine Learning with R\".</p><p>I\\'ve seen a guideline of', 136, b'2015-04-12T05:00:23.663', b'Is the R language suitable for Big Data', b'<bigdata><r>', 8, 1, 18, b'', 44, 118, b'2014-05-14T13:06:28.407', 0, b'', b'', b'')\n",
      "(28, 42, 2, b'2014-05-14T11:21:31.500', 5, 0, b'<p>NoSQL is a way to store data that does not require there to be some sort of relation. The simplicity of its design and horizontal scale-ability, one way they store data is the <code>key : value</code> pair design. This lends itself to proces', 59, b'2014-05-14T11:21:31.500', b'', b'', 0, 0, 0, b'', 0, 0, b'', 38, b'', b'', b'')\n",
      "(29, 43, 2, b'2014-05-14T11:23:25.913', 15, 0, b'<p><strong>Hadoop is not a database</strong>, hadoop is an entire ecosystem.</p><p><img src=\"https://i.stack.imgur.com/oOYp7.png\" alt=\"the hadoop ecosystem\"></p><p>Most people will refer to <a href=\"http://de.wikipedia.org/wiki/MapReduce\" rel=\"', 115, b'2015-05-18T12:30:19.497', b'', b'', 0, 0, 0, b'', 0, 115, b'2015-05-18T12:30:19.497', 38, b'', b'', b'')\n",
      "(30, 44, 2, b'2014-05-14T11:24:39.530', 40, 0, b\"<p>Actually this is coming around. In the book R in a Nutshell there is even a section on using R with Hadoop for big data processing. There are some work arounds that need to be done because R does all it's work in memory, so you are basically\", 59, b'2015-01-31T11:34:03.700', b'', b'', 0, 3, 0, b'', 0, 2522, b'2015-01-31T11:34:03.700', 41, b'', b'', b'')\n",
      "(31, 45, 2, b'2014-05-14T11:26:40.580', 6, 0, b'<p>First off, if your data has as many variations (in function of time, context, and others) as to make it hard to apply a single strategy to cope with it, you may be interested in doing a prior temporal/contextual/... characterization of the d', 84, b'2014-05-14T11:33:27.770', b'', b'', 0, 0, 0, b'', 0, 84, b'2014-05-14T11:33:27.770', 35, b'', b'', b'')\n",
      "(32, 46, 2, b'2014-05-14T12:32:29.503', 11, 0, b'<p>Note that there is an early version of LIBLINEAR ported to <a href=\"http://spark.apache.org\">Apache Spark</a>. See <a href=\"http://apache-spark-user-list.1001560.n3.nabble.com/Spark-LIBLINEAR-td5546.html\">mailing list comments</a> for some e', 21, b'2014-05-14T21:03:05.313', b'', b'', 0, 3, 0, b'', 0, 21, b'2014-05-14T21:03:05.313', 16, b'', b'', b'')\n",
      "(33, 47, 2, b'2014-05-14T12:39:41.197', 30, 0, b'<p>The main problem with using R for large data sets is the RAM constraint. The reason behind keeping all the data in RAM is that it provides much faster access and data manipulations than would storing on HDDs. If you are willing to take a hit', 62, b'2014-05-14T12:39:41.197', b'', b'', 0, 1, 0, b'', 0, 0, b'', 41, b'', b'', b'')\n",
      "(34, 48, 5, b'2014-05-14T13:08:26.647', 0, 0, b'<p><a href=\"http://www.r-project.org\" rel=\"nofollow\">R</a> is a language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at Bell Laboratories (fo', 201, b'2014-08-16T17:29:43.517', b'', b'', 0, 0, 0, b'', 0, 2961, b'2014-08-16T17:29:43.517', 0, b'', b'', b'')\n",
      "(35, 49, 4, b'2014-05-14T13:08:26.647', 0, 0, b'R is a free, open-source programming language and software environment for statistical computing, bioinformatics, and graphics.', 2961, b'2014-08-15T16:38:27.880', b'', b'', 0, 0, 0, b'', 0, 2961, b'2014-08-15T16:38:27.880', 0, b'', b'', b'')\n",
      "(36, 50, 1, b'2014-05-14T14:26:54.313', 6, 490, b'<p>I have an R script that generates a report based on the current contents of a database. This database is constantly in flux with records being added/deleted many times each day. How can I ask my computer to run this every night at 4 am so th', 151, b'2014-05-14T15:42:02.393', b'Running an R script programmatically', b'<r><databases><efficiency><tools>', 2, 1, 0, b'2017-08-29T10:27:14.067', 53, 0, b'', 0, b'', b'', b'')\n",
      "(37, 51, 2, b'2014-05-14T14:48:32.180', 9, 0, b'<blockquote>  <p>How can I ask my computer to run this every night at 4 am so that I have an up to date report waiting for me in the morning?</p></blockquote><p>You can set up a cronjob on a Linux system. These are run at the set time, if the c', 62, b'2014-05-14T14:57:32.243', b'', b'', 0, 2, 0, b'', 0, -1, b'2017-05-23T12:38:53.587', 50, b'', b'', b'')\n",
      "(38, 52, 1, b'2014-05-14T15:25:21.700', 33, 2922, b'<p>From my limited dabbling with data science using R, I realized that cleaning bad data is a very important part of preparing data for analysis. </p><p>Are there any best practices or processes for cleaning data before processing it? If so, ar', 157, b'2017-01-23T06:27:10.313', b'Organized processes to clean data', b'<r><data-cleaning>', 7, 2, 18, b'', 0, 136, b'2014-05-16T13:45:07.447', 0, b'', b'', b'')\n",
      "(39, 53, 2, b'2014-05-14T15:42:02.393', 10, 0, b'<p>For windows, use the task scheduler to set the task to run for example daily at 4:00 AM</p><p>It gives you many other options regarding frequency etc.<a href=\"http://en.wikipedia.org/wiki/Windows_Task_Scheduler\">http://en.wikipedia.org/wiki/', 116, b'2014-05-14T15:42:02.393', b'', b'', 0, 0, 0, b'', 0, 0, b'', 50, b'', b'', b'')\n",
      "(40, 57, 2, b'2014-05-14T16:29:39.927', 16, 0, b'<p>From my point of view, this question is suitable for a two-step answer. The first part, let us call it <em>soft preprocessing</em>, could be taken as the usage of different data mining algorithms to preprocess data in such a way that makes i', 84, b'2014-05-14T16:29:39.927', b'', b'', 0, 6, 0, b'', 0, 0, b'', 52, b'', b'', b'')\n",
      "(41, 58, 2, b'2014-05-14T17:06:33.337', 11, 0, b'<p>You can check out <a href=\"http://hunch.net/~vw/\">vowpal wabbit</a>. It is quite popular for large-scale learning and includes parallel provisions.</p><p>From their website:</p><blockquote>  <p>VW is the essence of speed in machine learning,', 119, b'2014-05-14T17:06:33.337', b'', b'', 0, 1, 0, b'', 0, 0, b'', 16, b'', b'', b'')\n",
      "(42, 59, 1, b'2014-05-14T17:48:21.240', 10, 964, b'', 158, b'2014-07-26T15:10:51.000', b\"What are R's memory constraints?\", b'<apache-hadoop><r>', 3, 1, 0, b'', 316, 62, b'2014-07-26T15:10:51.000', 0, b'', b'', b'')\n",
      "(43, 60, 2, b'2014-05-14T17:58:48.297', 7, 0, b\"<p>R performs all computation in-memory so you can't perform operation on a dataset that is larger than available RAM amount. However there are some libraries that allow bigdata processing using R and one of popular libraries for bigdata proces\", 118, b'2014-05-14T17:58:48.297', b'', b'', 0, 0, 0, b'', 0, 0, b'', 59, b'', b'', b'')\n",
      "(44, 61, 1, b'2014-05-14T18:09:01.940', 45, 7809, b'<p>Logic often states that by overfitting a model, its capacity to generalize is limited, though this might only mean that overfitting stops a model from improving after a certain complexity. Does overfitting cause models to become worse regard', 158, b'2017-09-17T02:27:31.110', b'Why Is Overfitting Bad in Machine Learning?', b'<machine-learning><predictive-modeling>', 8, 16, 17, b'', 62, -1, b'2017-04-13T12:50:41.230', 0, b'', b'', b'')\n",
      "(45, 62, 2, b'2014-05-14T18:27:56.043', 43, 0, b'<p>Overfitting is <em>empirically</em> bad.  Suppose you have a data set which you split in two, test and training.  An overfitted model is one that performs much worse on the test dataset than on training dataset.  It is often observed that mo', 26, b'2015-02-12T07:08:27.463', b'', b'', 0, 0, 0, b'', 0, 26, b'2015-02-12T07:08:27.463', 61, b'', b'', b'')\n",
      "(46, 64, 2, b'2014-05-14T18:37:52.333', 17, 0, b\"<p>Overfitting, in a nutshell, means take into account <strong>too much</strong> information from your data and/or prior knowledge, and use it in a model. To make it more straightforward, consider the following example: you're hired by some sci\", 84, b'2014-05-15T23:22:39.427', b'', b'', 0, 0, 0, b'', 0, 84, b'2014-05-15T23:22:39.427', 61, b'', b'', b'')\n",
      "(47, 65, 5, b'2014-05-14T18:45:23.917', 0, 0, b'', -1, b'2014-05-14T18:45:23.917', b'', b'', 0, 0, 0, b'', 0, -1, b'2014-05-14T18:45:23.917', 0, b'', b'', b'')\n",
      "(48, 66, 4, b'2014-05-14T18:45:23.917', 0, 0, b'Big data is the term for a collection of data sets so large and complex that it becomes difficult to process using on-hand database management tools or traditional data processing applications. The challenges include capture, curation, storage,', 118, b'2014-05-16T13:45:57.450', b'', b'', 0, 0, 0, b'', 0, 118, b'2014-05-16T13:45:57.450', 0, b'', b'', b'')\n",
      "(49, 67, 5, b'2014-05-14T18:48:42.263', 0, 0, b'', -1, b'2014-05-14T18:48:42.263', b'', b'', 0, 0, 0, b'', 0, -1, b'2014-05-14T18:48:42.263', 0, b'', b'', b'')\n",
      "(50, 68, 4, b'2014-05-14T18:48:42.263', 0, 0, b'', -1, b'2014-05-14T18:48:42.263', b'', b'', 0, 0, 0, b'', 0, -1, b'2014-05-14T18:48:42.263', 0, b'', b'', b'')\n",
      "(51, 69, 1, b'2014-05-14T20:03:15.233', 2, 80, b\"<p>First, think it's worth me stating what I mean by replication &amp; reproducibility:</p><ul><li>Replication of analysis A results in an exact copy of all inputs and processes that are supply and result in incidental outputs in analysis B.</l\", 158, b'2014-05-15T02:02:08.010', b'Is it possible to automate generating reproducibility documentation?', b'<processing>', 1, 2, 0, b'', 0, -1, b'2017-04-13T12:50:41.230', 0, b'', b'', b'')\n",
      "(52, 70, 2, b'2014-05-14T22:03:50.597', 2, 0, b'<p>To be reproducible without being just a replication, you would need to redo the experiment with new data, following the same technique as before.  The work flow is not as important as the techniques used.  Sample data in the same way, use th', 178, b'2014-05-14T22:03:50.597', b'', b'', 0, 4, 0, b'', 0, 0, b'', 69, b'', b'', b'')\n",
      "(53, 71, 1, b'2014-05-14T22:12:37.203', 13, 364, b'<p>What are the data conditions that we should watch out for, where p-values may not be the best way of deciding statistical significance?  Are there specific problem types that fall into this category?</p>', 179, b'2014-05-15T08:25:47.933', b'When are p-values deceptive?', b'<bigdata><statistics>', 3, 3, 3, b'', 84, 0, b'', 0, b'', b'', b'')\n",
      "(54, 72, 2, b'2014-05-14T22:40:40.363', 5, 0, b'<p>One algorithm that can be used for this is the <a href=\"http://en.wikipedia.org/wiki/K-means_clustering\" rel=\"noreferrer\">k-means clustering algorithm</a>.</p><p>Basically:</p><ol><li>Randomly choose k datapoints from your set, m_1, ..., m_k', 22, b'2014-05-14T22:40:40.363', b'', b'', 0, 0, 0, b'', 0, 0, b'', 31, b'', b'', b'')\n",
      "(55, 73, 2, b'2014-05-14T22:43:23.587', 4, 0, b'<p>You shouldn\\'t consider the p-value out of context.</p><p>One rather basic point (as illustrated by <a href=\"http://xkcd.com/882/\" rel=\"nofollow\">xkcd</a>) is that you need to consider how many tests you\\'re actually doing.  Obviously, you sho', 14, b'2014-05-14T22:43:23.587', b'', b'', 0, 0, 0, b'', 0, 0, b'', 71, b'', b'', b'')\n",
      "(56, 74, 2, b'2014-05-14T22:58:11.583', 2, 0, b'<p>One thing you should be aware of is the sample size you are using. Very large samples, such as economists using census data, will lead to deflated p-values. This paper <a href=\"http://galitshmueli.com/system/files/Print%20Version.pdf\" rel=\"n', 64, b'2014-05-14T22:58:11.583', b'', b'', 0, 0, 0, b'', 0, 0, b'', 71, b'', b'', b'')\n",
      "(57, 75, 1, b'2014-05-15T00:26:11.387', 4, 137, b'<p>If small p-values are plentiful in big data, what is a comparable replacement for p-values in data with million of samples?</p>', 158, b'2014-05-15T20:32:26.923', b'Is there a replacement for small p-values in big data?', b'<statistics>', 2, 0, 1, b'', 78, 0, b'', 0, b'', b'', b'')\n",
      "(58, 76, 1, b'2014-05-15T00:39:33.433', 6, 163, b'<p><em>(Note: Pulled this question from the <a href=\"http://area51.stackexchange.com/proposals/55053/data-science/57398#57398\">list of questions in Area51</a>, but believe the question is self explanatory. That said, believe I get the general i', 158, b'2014-05-18T15:18:08.050', b'Which Big Data technology stack is most suitable for processing tweets, extracting/expanding URLs and pushing (only) new links into 3rd party system?', b'<bigdata><tools><data-stream-mining>', 1, 6, 1, b'', 139, 118, b'2014-05-18T15:18:08.050', 0, b'', b'', b'')\n",
      "(59, 77, 1, b'2014-05-15T01:22:35.167', 9, 572, b'', 158, b'2015-05-10T21:18:01.617', b'Is this Neo4j comparison to RDBMS execution time correct?', b'<databases><nosql><neo4j>', 2, 0, 1, b'', 87, 118, b'2014-05-15T13:15:02.727', 0, b'', b'', b'')\n",
      "(60, 78, 2, b'2014-05-15T01:46:28.467', 5, 0, b'<p>There is no replacement in the strict sense of the word.  Instead you should look at other measures.</p><p>The other measures you look at depend on what you type of problem you are solving.  In general, if you have a small p-value, also cons', 178, b'2014-05-15T01:46:28.467', b'', b'', 0, 0, 0, b'', 0, -1, b'2017-04-13T12:50:41.230', 75, b'', b'', b'')\n",
      "(61, 79, 5, b'2014-05-15T03:19:40.360', 0, 0, b'<p>Conceptually speaking, <em>data-mining</em> can be thought of as one item (or set of skills and applications) in the toolkit of the data scientist.</p><p>More specifically, data-mining is an activity that seeks patterns in large, complex dat', 53, b'2017-08-27T17:25:18.230', b'', b'', 0, 0, 0, b'', 0, 3117, b'2017-08-27T17:25:18.230', 0, b'', b'', b'')\n",
      "(62, 80, 4, b'2014-05-15T03:19:40.360', 0, 0, b'An activity that seeks patterns in large, complex data sets. It usually emphasizes algorithmic techniques, but may also involve any set of related skills, applications, or methodologies with that goal.', 53, b'2014-05-16T13:46:05.850', b'', b'', 0, 0, 0, b'', 0, 53, b'2014-05-16T13:46:05.850', 0, b'', b'', b'')\n",
      "(63, 81, 1, b'2014-05-15T04:59:54.317', 15, 591, b'<p>What is(are) the difference(s) between parallel and distributed computing? When it comes to scalability and efficiency, it is very common to see solutions dealing with computations in clusters of machines, and sometimes it is referred to as ', 84, b'2014-05-18T17:38:01.383', b'Parallel and distributed computing', b'<definitions><parallel><distributed>', 3, 0, 0, b'', 82, 118, b'2014-05-15T09:31:51.370', 0, b'', b'', b'')\n",
      "(64, 82, 2, b'2014-05-15T05:19:34.757', 16, 0, b'<p>Simply set, \\'parallel\\' means running concurrently on distinct resources (CPUs), while \\'distributed\\' means running across distinct computers, involving issues related to networks.</p><p>Parallel computing using for instance <a href=\"http://en', 172, b'2014-05-15T05:25:39.970', b'', b'', 0, 0, 0, b'', 0, 172, b'2014-05-15T05:25:39.970', 81, b'', b'', b'')\n",
      "(65, 83, 2, b'2014-05-15T07:47:44.710', 8, 0, b'<p>I posted a pretty detailed answer on stackoverflow about when it is appropriate to use relational vs document (or NoSQL) database, here:  </p><p><strong><a href=\"https://stackoverflow.com/questions/13528216/motivations-for-using-relational-d', 26, b'2014-05-15T07:59:05.497', b'', b'', 0, 0, 0, b'', 0, -1, b'2017-05-23T12:38:53.587', 20, b'', b'', b'')\n",
      "(66, 84, 2, b'2014-05-15T08:19:40.577', 8, 0, b'<p>You are asking about <a href=\"http://en.wikipedia.org/wiki/Data_dredging\">Data Dredging</a>, which is what happens when testing a very large number of hypotheses against a data set, or testing hypotheses against a data set that were suggeste', 26, b'2014-05-15T08:25:47.933', b'', b'', 0, 1, 0, b'', 0, 26, b'2014-05-15T08:25:47.933', 71, b'', b'', b'')\n",
      "(67, 85, 2, b'2014-05-15T08:44:47.327', 3, 0, b'', 26, b'2014-05-15T20:32:26.923', b'', b'', 0, 1, 0, b'', 0, -1, b'2017-04-13T12:50:41.230', 75, b'', b'', b'')\n",
      "(68, 86, 1, b'2014-05-15T09:04:09.710', 12, 1451, b'<p>Given website access data in the form <code>session_id, ip, user_agent</code>, and optionally timestamp, following the conditions below, how would you best cluster the sessions into unique visitors?</p><p><code>session_id</code>: is an id gi', 116, b'2014-05-15T21:41:22.703', b'Clustering unique visitors by useragent, ip, session_id', b'<clustering>', 2, 0, 2, b'', 101, 116, b'2014-05-15T10:06:06.393', 0, b'', b'', b'')\n",
      "(69, 87, 2, b'2014-05-15T09:30:36.460', 7, 0, b'<p>Looking at this document called <a href=\"https://www.facebook.com/notes/facebook-data-team/anatomy-of-facebook/10150388519243859\">Anatomy of Facebook</a> I note that the median is 100. Looking at the cumulative function plot I can bet that t', 108, b'2014-05-15T09:30:36.460', b'', b'', 0, 0, 0, b'', 0, 0, b'', 77, b'', b'', b'')\n",
      "(70, 89, 1, b'2014-05-15T11:22:27.293', 10, 334, b'<p>For example, when searching something in Google, results return nigh-instantly.</p><p>I understand that Google sorts and indexes pages with algorithms etc., but I imagine it infeasible for the results of every single possible query to be ind', 189, b'2014-08-30T18:40:02.403', b'How does a query into a huge database return with negligible latency?', b'<bigdata><google><search>', 3, 0, 0, b'', 91, 189, b'2014-05-16T02:46:56.510', 0, b'', b'', b'')\n",
      "(71, 90, 2, b'2014-05-15T11:46:38.170', 4, 0, b'<p>The terms \"parallel computing\" and \"distributed computing\" certainly have a large overlap, but can be differentiated further. Actually, you already did this in your question, by later asking about \"parallel processing\" and \"distributed proce', 156, b'2014-05-15T11:46:38.170', b'', b'', 0, 0, 0, b'', 0, 0, b'', 81, b'', b'', b'')\n",
      "(72, 91, 2, b'2014-05-15T11:56:43.607', 12, 0, b\"<p>Well, I'm not sure if it is MapReduce that solves the problem, but it surely wouldn't be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it <em>feasible</em> to have \", 84, b'2014-05-16T04:33:52.310', b'', b'', 0, 2, 0, b'', 0, 84, b'2014-05-16T04:33:52.310', 89, b'', b'', b'')\n",
      "(73, 92, 2, b'2014-05-15T13:18:38.693', 9, 0, b'<p>MapReduce has nothing to do with real-time anything. It is a batch-oriented processing framework suitable for some offline tasks, like ETL and index building. Google has moved off of MapReduce for most jobs now, and even the Hadoop ecosystem', 21, b'2014-05-15T13:18:38.693', b'', b'', 0, 3, 0, b'', 0, 0, b'', 89, b'', b'', b'')\n",
      "(74, 93, 2, b'2014-05-15T13:30:04.270', 6, 0, b\"<p>There's not much you can do with just this data, but what little you can do does not rely on machine learning. </p><p>Yes, sessions from the same IP but different User-Agents are almost certainly distinct users. Sessions with the same IP and\", 21, b'2014-05-15T13:30:04.270', b'', b'', 0, 3, 0, b'', 0, 0, b'', 86, b'', b'', b'')\n",
      "(75, 94, 1, b'2014-05-15T14:41:24.020', 17, 223, b'<p>While building a rank, say for a search engine, or a recommendation system, is it valid to rely on click frequency to determine the relevance of an entry?</p>', 84, b'2015-11-23T15:36:28.760', b'Does click frequency account for relevance?', b'<recommender-system><information-retrieval>', 3, 0, 1, b'', 97, 0, b'', 0, b'', b'', b'')\n",
      "(76, 95, 2, b'2014-05-15T15:06:24.600', 5, 0, b'<p>Is it valid to <em>use</em> click frequency, then <strong>yes</strong>. Is it valid to use <strong>only</strong> the click frequency, then probably <strong>no</strong>.</p><p>Search relevance is much more complicated than just one metric. <a', 9, b'2014-05-15T15:06:24.600', b'', b'', 0, 0, 0, b'', 0, 0, b'', 94, b'', b'', b'')\n",
      "(77, 96, 2, b'2014-05-15T15:10:30.243', 7, 0, b'<p>For my part I can say that I use click frequency on i.e. eCommerce products. When you combine it with the days of the year it can even bring you great suggestions.</p><p>i.e.: We have historical data from 1 year over 2 products (Snowboots[],', 115, b'2015-11-23T15:36:28.760', b'', b'', 0, 0, 0, b'', 0, 115, b'2015-11-23T15:36:28.760', 94, b'', b'', b'')\n",
      "(78, 97, 2, b'2014-05-15T17:14:36.817', 14, 0, b'', 158, b'2014-05-15T23:08:04.300', b'', b'', 0, 0, 0, b'', 0, 158, b'2014-05-15T23:08:04.300', 94, b'', b'', b'')\n",
      "(79, 101, 2, b'2014-05-15T21:41:22.703', 8, 0, b'<p>One possibility here (and this is really an extension of what Sean Owen posted) is to define a \"stable user.\"</p><p>For the given info you have you can imagine making a user_id that is a hash of ip and some user agent info (pseudo code):</p>', 92, b'2014-05-15T21:41:22.703', b'', b'', 0, 2, 0, b'', 0, 0, b'', 86, b'', b'', b'')\n",
      "(80, 102, 1, b'2014-05-16T05:09:33.557', 6, 449, b\"<p>What is the best noSQL backend to use for a mobile game? Users can make a lot of servers requests, it needs also to retrieve users' historical records (like app purchasing) and analytics of usage behavior.</p>\", 199, b'2016-12-09T21:55:46.000', b'What is the Best NoSQL backend for a mobile game', b'<nosql><performance>', 1, 3, 0, b'', 111, 229, b'2014-05-18T19:41:19.157', 0, b'', b'', b'')\n",
      "(81, 103, 1, b'2014-05-16T14:26:12.270', 16, 3861, b'', 113, b'2014-06-25T15:53:30.723', b'Clustering based on similarity scores', b'<clustering><algorithms><similarity>', 4, 7, 2, b'', 0, 0, b'', 0, b'', b'', b'')\n",
      "(82, 104, 5, b'2014-05-16T15:35:51.420', 0, 0, b'<h2>Use the <a href=\"/questions/tagged/definitions\" class=\"post-tag\" title=\"show questions tagged &#39;definitions&#39;\" rel=\"tag\">definitions</a> tag when:</h2><p>You think we should create an official definition.</p><p>An existing Tag Wiki ne', 53, b'2014-05-20T13:50:52.447', b'', b'', 0, 0, 0, b'', 0, 53, b'2014-05-20T13:50:52.447', 0, b'', b'', b'')\n",
      "(83, 105, 4, b'2014-05-16T15:35:51.420', 0, 0, b'a discussion (meta) tag used when there exists *disagreement* or *confusion* about the everyday meaning of a term or phrase.', 53, b'2014-05-20T13:53:05.697', b'', b'', 0, 0, 0, b'', 0, 53, b'2014-05-20T13:53:05.697', 0, b'', b'', b'')\n",
      "(84, 106, 2, b'2014-05-16T16:25:58.250', 6, 0, b'<p>There are many overlaps between data mining and datascience. I would say that people with the role of datamining are concerned with data collection and the extraction of features from unfiltered, unorganised and mostly raw/wild datasets. Som', 34, b'2014-05-16T16:25:58.250', b'', b'', 0, 0, 0, b'', 0, 0, b'', 14, b'', b'', b'')\n",
      "(85, 107, 1, b'2014-05-16T20:07:50.983', 11, 103, b'<p>Consider a stream containing <a href=\"http://en.m.wikipedia.org/wiki/Tuple\" rel=\"nofollow\">tuples</a> <code>(user, new_score)</code> representing users\\' scores in an online game. The stream could have 100-1,000 new elements per second. The g', 200, b'2014-05-19T07:33:50.080', b'Opensource tools for help in mining stream of leader board scores', b'<tools><data-stream-mining>', 2, 0, 0, b'', 0, 118, b'2014-05-19T07:33:50.080', 0, b'', b'', b'')\n",
      "(86, 108, 5, b'2014-05-16T20:24:38.980', 1, 0, b'', -1, b'2014-05-16T20:24:38.980', b'', b'', 0, 0, 0, b'', 0, -1, b'2014-05-16T20:24:38.980', 0, b'', b'', b'')\n",
      "(87, 109, 4, b'2014-05-16T20:24:38.980', 0, 0, b'An activity that seeks patterns in a continuous stream of data elements, usually involving summarizing the stream in some way.', 200, b'2014-05-20T13:52:00.620', b'', b'', 0, 0, 0, b'', 0, 200, b'2014-05-20T13:52:00.620', 0, b'', b'', b'')\n",
      "(88, 111, 2, b'2014-05-17T03:07:59.707', 8, 0, b'<p>Some factors you might consider:</p><p>Developer familiarity: go with whatever you or your developers are familiar with.  Mongo, Couch, Riak, DynamoDB etc all have their strengths but all should do ok here, so rather than going for an unfami', 26, b'2014-05-18T06:43:52.453', b'', b'', 0, 1, 0, b'', 0, 26, b'2014-05-18T06:43:52.453', 102, b'', b'', b'')\n",
      "(89, 112, 2, b'2014-05-17T04:18:10.020', 8, 0, b'<p>This isn\\'t a full solution, but you may want to look into <a href=\"http://www.orientechnologies.com/\">OrientDB</a> as part of your stack. Orient is a Graph-Document database server written entirely in Java. </p><p>In graph databases, relatio', 70, b'2014-05-17T04:18:10.020', b'', b'', 0, 1, 0, b'', 0, 0, b'', 107, b'', b'', b'')\n",
      "(90, 113, 1, b'2014-05-17T04:53:03.913', 14, 176, b'<p>When a relational database, like MySQL, has better performance than a no relational, like MongoDB?</p><p>I saw a question on Quora other day, about why Quora still uses MySQL as their backend, and that their performance is still good.</p>', 199, b'2017-06-05T19:30:23.440', b'When a relational database has better performance than a no relational', b'<bigdata><performance><databases><nosql>', 1, 2, 0, b'', 122, 31513, b'2017-06-05T19:30:23.440', 0, b'', b'', b'')\n",
      "(91, 115, 1, b'2014-05-17T08:45:08.420', 10, 1135, b'<p>If I have a very long list of paper names, how could I get abstract of these papers from internet or any database?</p><p>The paper names are like \"Assessment of Utility in Web Mining for the Domain of Public Health\".</p><p>Does any one know ', 212, b'2014-05-18T06:54:08.560', b'Is there any APIs for crawling abstract of paper?', b'<data-mining><machine-learning>', 2, 5, 1, b'', 131, 0, b'', 0, b'', b'', b'')\n",
      "(92, 116, 1, b'2014-05-17T09:16:18.823', 26, 2666, b\"<p>I have a database from my Facebook application and I am trying to use machine learning to estimate users' age based on what Facebook sites they like.</p><p>There are three crucial characteristics of my database:</p><ul><li><p>the age distrib\", 173, b'2016-04-28T06:18:44.780', b\"Machine learning techniques for estimating users' age based on Facebook sites they like\", b'<machine-learning><dimensionality-reduction><python>', 6, 3, 10, b'', 121, 173, b'2014-05-17T19:26:53.783', 0, b'', b'', b'')\n",
      "(93, 118, 5, b'2014-05-17T13:41:20.283', 0, 0, b'<p><strong>NoSQL</strong> (sometimes expanded to \"not only <a href=\"/questions/tagged/sql\" class=\"post-tag\" title=\"show questions tagged &#39;sql&#39;\" rel=\"tag\">sql</a>\") is a broad class of database management systems that differ from the cla', 201, b'2017-08-27T17:25:05.257', b'', b'', 0, 0, 0, b'', 0, 381, b'2017-08-27T17:25:05.257', 0, b'', b'', b'')\n",
      "(94, 119, 4, b'2014-05-17T13:41:20.283', 0, 0, b'', -1, b'2014-05-17T13:41:20.283', b'', b'', 0, 0, 0, b'', 0, -1, b'2014-05-17T13:41:20.283', 0, b'', b'', b'')\n",
      "(95, 120, 2, b'2014-05-17T18:15:11.937', 5, 0, b'<p>arXiv has an <a href=\"http://arxiv.org/help/bulk_data\" rel=\"noreferrer\">API and bulk download</a> but if you want something for paid journals it will be hard to come by without paying an indexer like pubmed or elsevier or the like.</p>', 92, b'2014-05-17T18:15:11.937', b'', b'', 0, 1, 0, b'', 0, 0, b'', 115, b'', b'', b'')\n",
      "(96, 121, 2, b'2014-05-17T18:53:30.123', 17, 0, b'<p>One thing to start off with would be k-NN.  The idea here is that you have a user/item matrix and for some of the users you have a reported age.  The age for a person in the user item matrix might be well determined by something like the mea', 92, b'2014-05-17T18:53:30.123', b'', b'', 0, 2, 0, b'', 0, 0, b'', 116, b'', b'', b'')\n",
      "(97, 122, 2, b'2014-05-17T20:56:15.577', 10, 0, b\"<p>It depends on your data and what you're doing with it. For example, if the processing you have to do requires transactions to synchronize across nodes, it will likely be faster to use transactions implemented in an RDBMS rather than implemen\", 180, b'2014-05-17T20:56:15.577', b'', b'', 0, 0, 0, b'', 0, 0, b'', 113, b'', b'', b'')\n",
      "(98, 123, 5, b'2014-05-17T21:10:41.990', 0, 0, b'<p>The most basic relationship to describe is a <strong>linear relationship</strong> between variables, <em>x</em> and <em>y</em>, such that they can be said to be highly-correlated when every increase in <em>x</em> results in a proportional in', 53, b'2014-05-20T13:50:21.763', b'', b'', 0, 0, 0, b'', 0, 53, b'2014-05-20T13:50:21.763', 0, b'', b'', b'')\n",
      "(99, 124, 4, b'2014-05-17T21:10:41.990', 0, 0, b'A statistics term used to describe a type of dependence between variables (or data sets). Correlations are often used as an indicator of predictability. However, correlation does NOT imply causation. Different methods of calculating correlation', 53, b'2014-05-20T13:50:19.543', b'', b'', 0, 0, 0, b'', 0, 53, b'2014-05-20T13:50:19.543', 0, b'', b'', b'')\n",
      "(100, 125, 1, b'2014-05-17T21:52:34.563', 1, 338, b\"<p>I want learn about NoSQL and when is better to use SQL or NoSQL. I know that this question depends on the case, but I'm asking for a good documentation on NoSQL, and some explanation of when is better to use SQL or NoSQL (use cases, etc). Al\", 109, b'2016-04-11T22:08:14.620', b'How to learn noSQL databases and how to know when SQL or noSQL is better', b'<nosql>', 2, 0, 1, b'2014-05-21T14:00:22.100', 126, 84, b'2014-05-18T16:10:50.820', 0, b'', b'', b'')\n"
     ]
    }
   ],
   "source": [
    "for post in dataset_h5['table']:\n",
    "    print(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pandas to import H5 data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_hdf = pd.read_hdf('./files/posts-100.h5','posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body',\n",
       "       'OwnerUserId', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount',\n",
       "       'CommentCount', 'FavoriteCount', 'ClosedDate', 'AcceptedAnswerId',\n",
       "       'LastEditorUserId', 'LastEditDate', 'ParentId', 'CommunityOwnedDate',\n",
       "       'LastEditorDisplayName', 'OwnerDisplayName'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_hdf.columns  # equivalent to posts_hdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-05-14T00:36:31.077</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-05-14T00:53:43.273</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-05-14T01:25:59.677</td>\n",
       "      <td>Is Data Science the Same as Data Mining?</td>\n",
       "      <td>&lt;data-mining&gt;&lt;definitions&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CreationDate                                     Title  \\\n",
       "2  2014-05-14T00:36:31.077                                             \n",
       "3  2014-05-14T00:53:43.273                                             \n",
       "4  2014-05-14T01:25:59.677  Is Data Science the Same as Data Mining?   \n",
       "\n",
       "                         Tags  \n",
       "2                              \n",
       "3                              \n",
       "4  <data-mining><definitions>  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use arguments for start, stop, columns\n",
    "pd.read_hdf('./files/posts-100.h5','posts',start=2,stop=5,columns=['CreationDate','Title','Tags']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>&lt;machine-learning&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>&lt;data-mining&gt;&lt;definitions&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18</td>\n",
       "      <td>&lt;machine-learning&gt;&lt;bigdata&gt;&lt;libsvm&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>73</td>\n",
       "      <td>&lt;bigdata&gt;&lt;scalability&gt;&lt;efficiency&gt;&lt;performance&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                             Tags\n",
       "0      9                               <machine-learning>\n",
       "3     12                                                 \n",
       "4     21                       <data-mining><definitions>\n",
       "6     18              <machine-learning><bigdata><libsvm>\n",
       "9     73  <bigdata><scalability><efficiency><performance>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more arguments: mode, where\n",
    "pd.read_hdf('./files/posts-100.h5','posts',mode='r',columns=['Score','Tags'],where='Score>10 or Tags=\"<machine-learning>\"').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing MATLAB (MATrix LABoratory) data using SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_mat = scipy.io.loadmat('./files/posts-100.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(posts_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'posts'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(posts_mat['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(array([[  0.,  10.,   0.,   0.,  29.,   0.,  46.,   0.,   0.,  37.,  26.,\n",
       "          0.,  24.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  72.,\n",
       "          0.,   0.,   0.,  43.,   0.,  44.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  53.,   0.,   0.,   0.,   0.,   0., 316.,   0.,\n",
       "         62.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  84.,   0.,\n",
       "          0.,   0.,  78., 139.,  87.,   0.,   0.,   0.,  82.,   0.,   0.,\n",
       "          0.,   0., 101.,   0.,  91.,   0.,   0.,   0.,   0.,  97.,   0.,\n",
       "          0.,   0.,   0., 111.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0., 122., 131., 121.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.]]), array([[ 1.,  3.,  0.,  0.,  4.,  0.,  2.,  0.,  0., 12.,  5.,  0.,  9.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  3.,  0.,  2.,\n",
       "         0.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  7.,\n",
       "         0.,  0.,  0.,  3.,  0.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  3.,  0.,  0.,  0.,  2.,  1.,  2.,  0.,  0.,  0.,  3.,  0.,\n",
       "         0.,  0.,  0.,  2.,  0.,  3.,  0.,  0.,  0.,  0.,  3.,  0.,  0.,\n",
       "         0.,  0.,  1.,  4.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  1.,\n",
       "         2.,  6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([[ 1,  4,  0,  1,  1,  1,  0,  0,  0,  5,  1,  0,  3,  0,  9,  0,\n",
       "         0,  0,  0,  0,  3,  4,  0,  0,  4,  2,  0,  1,  0,  0,  3,  0,\n",
       "         3,  1,  0,  0,  1,  2,  2,  0,  6,  1,  1,  0, 16,  0,  0,  0,\n",
       "         0,  0,  0,  2,  4,  3,  0,  0,  0,  0,  6,  0,  0,  0,  0,  0,\n",
       "         0,  0,  1,  1,  0,  0,  0,  0,  2,  3,  3,  0,  0,  0,  0,  2,\n",
       "         3,  7,  0,  0,  0,  0,  0,  0,  1,  1,  2,  5,  3,  0,  0,  1,\n",
       "         2,  0,  0,  0]]), array([[ 1.,  1.,  0.,  0.,  4.,  0.,  0.,  0.,  0., 20.,  1.,  0., 80.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  3.,  0.,  2.,\n",
       "         0., 18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 18.,\n",
       "         0.,  0.,  0.,  0.,  0., 17.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  3.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([[  5,   7,   9,  10,  14,  15,  16,  17,  18,  19,  20,  21,  22,\n",
       "         23,  24,  25,  26,  27,  28,  29,  30,  31,  33,  35,  37,  38,\n",
       "         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "         53,  57,  58,  59,  60,  61,  62,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,\n",
       "         83,  84,  85,  86,  87,  89,  90,  91,  92,  93,  94,  95,  96,\n",
       "         97, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113,\n",
       "        115, 116, 118, 119, 120, 121, 122, 123, 124]]), array([[0.0000e+00, 9.7000e+01, 0.0000e+00, 0.0000e+00, 3.2200e+02,\n",
       "        0.0000e+00, 8.4000e+01, 6.3000e+01, 0.0000e+00, 1.0119e+04,\n",
       "        0.0000e+00, 0.0000e+00, 9.7000e+01, 0.0000e+00, 1.4000e+01,\n",
       "        0.0000e+00, 1.1500e+02, 1.4000e+01, 0.0000e+00, 0.0000e+00,\n",
       "        2.6000e+01, 2.4000e+01, 1.3200e+02, 0.0000e+00, 5.1450e+04,\n",
       "        1.3400e+02, 0.0000e+00, 1.1800e+02, 0.0000e+00, 1.1500e+02,\n",
       "        2.5220e+03, 8.4000e+01, 2.1000e+01, 0.0000e+00, 2.9610e+03,\n",
       "        2.9610e+03, 0.0000e+00, 0.0000e+00, 1.3600e+02, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 6.2000e+01, 0.0000e+00, 0.0000e+00,\n",
       "        2.6000e+01, 8.4000e+01, 0.0000e+00, 1.1800e+02, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1800e+02, 1.1800e+02,\n",
       "        0.0000e+00, 3.1170e+03, 5.3000e+01, 1.1800e+02, 1.7200e+02,\n",
       "        0.0000e+00, 2.6000e+01, 0.0000e+00, 1.1600e+02, 0.0000e+00,\n",
       "        1.8900e+02, 0.0000e+00, 8.4000e+01, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.1500e+02, 1.5800e+02, 0.0000e+00,\n",
       "        2.2900e+02, 0.0000e+00, 5.3000e+01, 5.3000e+01, 0.0000e+00,\n",
       "        1.1800e+02, 0.0000e+00, 2.0000e+02, 2.6000e+01, 0.0000e+00,\n",
       "        3.1513e+04, 0.0000e+00, 1.7300e+02, 3.8100e+02, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3000e+01, 5.3000e+01]]), array([[   5.,   36.,   51.,   22.,   66.,   64.,   63.,   63.,    0.,\n",
       "          84.,   96.,   14.,   97.,   97.,   14.,  104.,  115.,  108.,\n",
       "         118.,   53.,   26.,  118.,  132.,   26.,    9.,  134.,  104.,\n",
       "         136.,   59.,  115.,   59.,   84.,   21.,   62.,  201., 2961.,\n",
       "         151.,   62.,  157.,  116.,   84.,  119.,  158.,  118.,  158.,\n",
       "          26.,   84.,    0.,  118.,    0.,    0.,  158.,  178.,  179.,\n",
       "          22.,   14.,   64.,  158.,  158.,  158.,  178.,   53.,   53.,\n",
       "          84.,  172.,   26.,   26.,   26.,  116.,  108.,  189.,  156.,\n",
       "          84.,   21.,   21.,   84.,    9.,  115.,  158.,   92.,  199.,\n",
       "         113.,   53.,   53.,   34.,  200.,    0.,  200.,   26.,   70.,\n",
       "         199.,  212.,  173.,  201.,    0.,   92.,   92.,  180.,   53.,\n",
       "          53.]]), array([[  0.,   0.,   5.,   7.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         19.,   0.,   7.,  22.,  19.,  20.,  20.,   7.,  14.,  19.,   0.,\n",
       "         20.,   0.,  19.,   0.,  20.,   0.,  38.,  38.,  41.,  35.,  16.,\n",
       "         41.,   0.,   0.,   0.,  50.,   0.,  50.,  52.,  16.,   0.,  59.,\n",
       "          0.,  61.,  61.,   0.,   0.,   0.,   0.,   0.,  69.,   0.,  31.,\n",
       "         71.,  71.,   0.,   0.,   0.,  75.,   0.,   0.,   0.,  81.,  20.,\n",
       "         71.,  75.,   0.,  77.,   0.,  81.,  89.,  89.,  86.,   0.,  94.,\n",
       "         94.,  94.,  86.,   0.,   0.,   0.,   0.,  14.,   0.,   0.,   0.,\n",
       "        102., 107.,   0.,   0.,   0.,   0.,   0., 115., 116., 113.,   0.,\n",
       "          0.]]), array([[1, 1, 2, 2, 1, 1, 1, 5, 4, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1,\n",
       "        2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 5, 4, 1, 2, 1, 2, 2, 2, 1, 2,\n",
       "        1, 2, 2, 5, 4, 5, 4, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 5, 4, 1, 2, 2,\n",
       "        2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 5, 4, 2, 1, 5, 4,\n",
       "        2, 2, 1, 1, 1, 5, 4, 2, 2, 2, 5, 4]]), array([[ 9.,  4.,  5., 12., 21.,  2., 18.,  0.,  0., 73., 17., 29., 99.,\n",
       "         8., 89.,  7., 14., 10.,  6., 24., 20.,  9.,  6., 18., 77., 14.,\n",
       "         8., 43.,  5., 15., 40.,  6., 11., 30.,  0.,  0.,  6.,  9., 33.,\n",
       "        10., 16., 11., 10.,  7., 45., 43., 17.,  0.,  0.,  0.,  0.,  2.,\n",
       "         2., 13.,  5.,  4.,  2.,  4.,  6.,  9.,  5.,  0.,  0., 15., 16.,\n",
       "         8.,  8.,  3., 12.,  7., 10.,  4., 12.,  9.,  6., 17.,  5.,  7.,\n",
       "        14.,  8.,  6., 16.,  0.,  0.,  6., 11.,  1.,  0.,  8.,  8., 14.,\n",
       "        10., 26.,  0.,  0.,  5., 17., 10.,  0.,  0.]]), array([[4.48000e+02, 3.88000e+02, 0.00000e+00, 0.00000e+00, 1.24300e+03,\n",
       "        5.43000e+02, 3.22000e+02, 0.00000e+00, 0.00000e+00, 7.99300e+03,\n",
       "        3.11000e+02, 0.00000e+00, 1.05748e+05, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 1.19000e+03, 0.00000e+00, 3.46000e+02, 0.00000e+00,\n",
       "        2.98500e+03, 0.00000e+00, 4.57100e+03, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 4.90000e+02, 0.00000e+00, 2.92200e+03, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 9.64000e+02, 0.00000e+00, 7.80900e+03,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 8.00000e+01, 0.00000e+00, 3.64000e+02, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 1.37000e+02, 1.63000e+02, 5.72000e+02,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 5.91000e+02, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 1.45100e+03, 0.00000e+00,\n",
       "        3.34000e+02, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        2.23000e+02, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        4.49000e+02, 3.86100e+03, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        1.03000e+02, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        1.76000e+02, 1.13500e+03, 2.66600e+03, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00]]))]],\n",
       "      dtype=[('AcceptedAnswerId', 'O'), ('AnswerCount', 'O'), ('CommentCount', 'O'), ('FavoriteCount', 'O'), ('Id', 'O'), ('LastEditorUserId', 'O'), ('OwnerUserId', 'O'), ('ParentId', 'O'), ('PostTypeId', 'O'), ('Score', 'O'), ('ViewCount', 'O')])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_mat['posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_mat['posts'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_mat['posts'].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Pickle data using pickle package and pandas\n",
    "\n",
    "Pickle ([doc](https://docs.python.org/3/library/pickle.html)):\n",
    "* User for serializing and deserializing objects\n",
    "* Convenient, binary format\n",
    "* Python specific\n",
    "* Supports compression\n",
    "\n",
    "Using the pickle module:\n",
    "* Open the file, using 'rb'  (read, binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./files/posts-100.pkl.gz','rb') as pickle_file:\n",
    "    df_pickle = pickle.load(pickle_file)\n",
    "    \n",
    "# original object that was serialized to the pickle file was a dataframe, so the result of load is the original dataframe\n",
    "type(df_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>...</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "      <th>OwnerDisplayName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-05-13T23:58:30.457</td>\n",
       "      <td>9</td>\n",
       "      <td>448</td>\n",
       "      <td>&lt;p&gt;I've always been interested in machine lear...</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-05-14T00:36:31.077</td>\n",
       "      <td>How can I do simple machine learning without h...</td>\n",
       "      <td>&lt;machine-learning&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-05-14T14:40:25.950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  PostTypeId             CreationDate  Score  ViewCount  \\\n",
       "0   5           1  2014-05-13T23:58:30.457      9        448   \n",
       "\n",
       "                                                Body  OwnerUserId  \\\n",
       "0  <p>I've always been interested in machine lear...            5   \n",
       "\n",
       "          LastActivityDate                                              Title  \\\n",
       "0  2014-05-14T00:36:31.077  How can I do simple machine learning without h...   \n",
       "\n",
       "                 Tags       ...         CommentCount  FavoriteCount  \\\n",
       "0  <machine-learning>       ...                    1              1   \n",
       "\n",
       "                ClosedDate AcceptedAnswerId  LastEditorUserId  LastEditDate  \\\n",
       "0  2014-05-14T14:40:25.950                0                 0                 \n",
       "\n",
       "  ParentId  CommunityOwnedDate LastEditorDisplayName OwnerDisplayName  \n",
       "0        0                                                             \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pickle.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body',\n",
       "       'OwnerUserId', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount',\n",
       "       'CommentCount', 'FavoriteCount', 'ClosedDate', 'AcceptedAnswerId',\n",
       "       'LastEditorUserId', 'LastEditDate', 'ParentId', 'CommunityOwnedDate',\n",
       "       'LastEditorDisplayName', 'OwnerDisplayName'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pickle.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to load pickle file\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>Id</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AcceptedAnswerId AnswerCount CommentCount FavoriteCount Id LastEditorUserId  \\\n",
       "0              NaN           1            1             1  5              NaN   \n",
       "\n",
       "  OwnerUserId ParentId PostTypeId Score ViewCount  \n",
       "0           5      NaN          1     9       448  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_df = pd.read_pickle('./files/posts-100.pkl')\n",
    "pickle_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
